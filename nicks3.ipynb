{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T03:29:09.369576Z",
          "iopub.status.busy": "2022-03-16T03:29:09.369229Z",
          "iopub.status.idle": "2022-03-16T03:29:10.073884Z",
          "shell.execute_reply": "2022-03-16T03:29:10.073055Z",
          "shell.execute_reply.started": "2022-03-16T03:29:09.369545Z"
        },
        "id": "eI3YQTUCD8uu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "from IPython.display import Image, Audio\n",
        "from midi2audio import FluidSynth\n",
        "from music21 import corpus, converter, instrument, note, stream, chord, duration\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import midi2audio\n",
        "import music21\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from music21 import converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from keras.layers import LSTM, Input, Dropout, Dense, Activation, Embedding, Concatenate, Reshape, GlobalAveragePooling1D\n",
        "from keras.layers import Flatten, RepeatVector, Permute, TimeDistributed\n",
        "from keras.layers import Multiply, Lambda, Softmax\n",
        "import keras.backend as K \n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.execute_input": "2022-03-16T02:24:31.833094Z",
          "iopub.status.busy": "2022-03-16T02:24:31.832743Z",
          "iopub.status.idle": "2022-03-16T02:24:45.971576Z",
          "shell.execute_reply": "2022-03-16T02:24:45.970622Z",
          "shell.execute_reply.started": "2022-03-16T02:24:31.833058Z"
        },
        "id": "QzY5xFT5D8uv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# fs = FluidSynth(sound_font='/Users/nicholasbarsi-rhyne/.fluidsynt/FluidR3_GM.sf2')\n",
        "fs = FluidSynth()\n",
        "file = 'db2/bach/bach_846.mid'\n",
        "fs.midi_to_audio(file, 'bach_846.wav')\n",
        "IPython.display.Audio(\"bach_846.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbxTwu9mD8uv"
      },
      "source": [
        "---------------------\n",
        "# Extracting the data\n",
        "\n",
        "It loops through the score and extracts the pitch and time of each note (and rest) into two lists. The entire chord is stored as a string, and individual notes in the chord are separated by dots. The male after the name of each note refers to the octave to which the note belongs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQmTAHt2D8uv"
      },
      "source": [
        "## Defiing Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def helper_function(x):\n",
        "    return tf.reduce_sum(x, axis=1)\n",
        "\n",
        "def get_music_list(data_folder):\n",
        "    file_list = []\n",
        "    \n",
        "    # Walk through all subdirectories\n",
        "    for root, _, _ in os.walk(data_folder):\n",
        "        # Find all MIDI files in current directory\n",
        "        midi_files = glob.glob(os.path.join(root, \"*.mid\"))\n",
        "        # Add them to our list\n",
        "        file_list.extend(midi_files)\n",
        "    \n",
        "    parser = converter\n",
        "    return file_list, parser\n",
        "\n",
        "def create_network(n_notes, n_durations, embed_size = 100, rnn_units = 256, use_attention = False):\n",
        "    notes_in = Input(shape = (None,))\n",
        "    durations_in = Input(shape = (None,))\n",
        "\n",
        "    x1 = Embedding(n_notes, embed_size)(notes_in)\n",
        "    x2 = Embedding(n_durations, embed_size)(durations_in) \n",
        "    x = Concatenate()([x1,x2])\n",
        "    x = LSTM(rnn_units, return_sequences=True)(x)\n",
        "\n",
        "    if use_attention:\n",
        "        x = LSTM(rnn_units, return_sequences=True)(x)\n",
        "        e = Dense(1, activation='tanh')(x)\n",
        "        e = Reshape([-1])(e)\n",
        "        alpha = Activation('softmax')(e)\n",
        "        alpha_repeated = Permute([2, 1])(RepeatVector(rnn_units)(alpha))\n",
        "        c = Multiply()([x, alpha_repeated])\n",
        "        c = Lambda(helper_function)(c)    \n",
        "    else:\n",
        "        c = LSTM(rnn_units)(x)\n",
        "                                    \n",
        "    notes_out = Dense(n_notes, activation = 'softmax', name = 'pitch')(c)\n",
        "    durations_out = Dense(n_durations, activation = 'softmax', name = 'duration')(c)\n",
        "   \n",
        "    model = Model([notes_in, durations_in], [notes_out, durations_out])    \n",
        "\n",
        "    if use_attention:\n",
        "        att_model = Model([notes_in, durations_in], alpha)\n",
        "    else:\n",
        "        att_model = None\n",
        "        \n",
        "    opti = RMSprop(learning_rate = 0.001)\n",
        "    model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy'], optimizer=opti)\n",
        "\n",
        "    return model, att_model\n",
        "\n",
        "\n",
        "def get_distinct(elements):\n",
        "    # Get all pitch names\n",
        "    element_names = sorted(set(elements))\n",
        "    n_elements = len(element_names)\n",
        "    return (element_names, n_elements)\n",
        "\n",
        "def create_lookups(element_names):\n",
        "    # create dictionary to map notes and durations to integers\n",
        "    element_to_int = dict((element, number) for number, element in enumerate(element_names))\n",
        "    int_to_element = dict((number, element) for number, element in enumerate(element_names))\n",
        "    return (element_to_int, int_to_element)    \n",
        "\n",
        "def prepare_sequences(notes, durations, lookups, distincts, seq_len =32):\n",
        "    note_to_int, int_to_note, duration_to_int, int_to_duration = lookups\n",
        "    note_names, n_notes, duration_names, n_durations = distincts\n",
        "\n",
        "    notes_network_input = []\n",
        "    notes_network_output = []\n",
        "    durations_network_input = []\n",
        "    durations_network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(len(notes) - seq_len):\n",
        "        notes_sequence_in = notes[i:i + seq_len]\n",
        "        notes_sequence_out = notes[i + seq_len]\n",
        "        notes_network_input.append([note_to_int[char] for char in notes_sequence_in])\n",
        "        notes_network_output.append(note_to_int[notes_sequence_out])\n",
        "\n",
        "        durations_sequence_in = durations[i:i + seq_len]\n",
        "        durations_sequence_out = durations[i + seq_len]\n",
        "        durations_network_input.append([duration_to_int[char] for char in durations_sequence_in])\n",
        "        durations_network_output.append(duration_to_int[durations_sequence_out])\n",
        "\n",
        "    n_patterns = len(notes_network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    notes_network_input = np.reshape(notes_network_input, (n_patterns, seq_len))\n",
        "    durations_network_input = np.reshape(durations_network_input, (n_patterns, seq_len))\n",
        "    network_input = [notes_network_input, durations_network_input]\n",
        "\n",
        "    notes_network_output = to_categorical(notes_network_output, num_classes=n_notes)\n",
        "    durations_network_output = to_categorical(durations_network_output, num_classes=n_durations)\n",
        "    network_output = [notes_network_output, durations_network_output]\n",
        "    return (network_input, network_output)\n",
        "\n",
        "def sample_with_temp(preds, temperature):\n",
        "    if temperature == 0:\n",
        "        return np.argmax(preds)\n",
        "    else:\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        return np.random.choice(len(preds), p=preds)\n",
        "\n",
        "\n",
        "data_folder ='db2'\n",
        "\n",
        "mode = 'build'\n",
        "\n",
        "# data params\n",
        "intervals = range(1)\n",
        "seq_len = 32\n",
        "\n",
        "# model params\n",
        "embed_size = 100\n",
        "rnn_units = 256\n",
        "use_attention = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "if mode == 'build':    \n",
        "    music_list, parser = get_music_list(data_folder)\n",
        "    print(len(music_list), 'files in total')\n",
        "\n",
        "    notes = []\n",
        "    durations = []\n",
        "    pbar = tqdm(music_list, desc=\"Processing files\")\n",
        "    for i, file in enumerate(pbar):\n",
        "        original_score = parser.parse(file).chordify()        \n",
        "        for interval in intervals:\n",
        "            score = original_score.transpose(interval)\n",
        "\n",
        "            notes.extend(['START'] * seq_len)\n",
        "            durations.extend([0]* seq_len)\n",
        "\n",
        "            for element in score.flatten(): \n",
        "                if isinstance(element, note.Note):\n",
        "                    if element.isRest:\n",
        "                        notes.append(str(element.name))\n",
        "                        if element.duration.quarterLength < .5 and element.duration.quarterLength != 0:\n",
        "                            durations.append(1.0)    \n",
        "                        else:\n",
        "                            durations.append(element.duration.quarterLength)\n",
        "                    else:\n",
        "                        notes.append(str(element.name))\n",
        "                        if element.duration.quarterLength < .5 and element.duration.quarterLength != 0:\n",
        "                            durations.append(1.0) \n",
        "                        else:\n",
        "                            durations.append(element.duration.quarterLength)\n",
        "                        \n",
        "                if isinstance(element, chord.Chord):\n",
        "                    \n",
        "                    if len(element.pitches) > 2:\n",
        "                        # For complex chords with more than 3 notes, just use the first 3 notes\n",
        "                        notes.append('.'.join(n.nameWithOctave for n in element.pitches[:2]))\n",
        "                    else:\n",
        "                        # For simpler chords (3 or fewer notes), keep the full representation\n",
        "                        notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n",
        "                    if element.duration.quarterLength < .5 and element.duration.quarterLength != 0:\n",
        "                            durations.append(1.0) \n",
        "                    else:\n",
        "                        durations.append(element.duration.quarterLength)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CExQQr3OD8uw"
      },
      "source": [
        "------------------------------------------\n",
        "# Embedding Note and Duration\n",
        "\n",
        "To create a dataset for training the model, we first convert the pitch and tempo into integer values. It doesn't matter what these values are because we use an embedding layer to convert the integer to a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T02:25:42.334363Z",
          "iopub.status.busy": "2022-03-16T02:25:42.334175Z",
          "iopub.status.idle": "2022-03-16T02:25:42.352485Z",
          "shell.execute_reply": "2022-03-16T02:25:42.351542Z",
          "shell.execute_reply.started": "2022-03-16T02:25:42.33434Z"
        },
        "id": "6gx9PvvqD8uw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# get the distinct sets of notes and durations\n",
        "note_names, n_notes = get_distinct(notes)\n",
        "duration_names, n_durations = get_distinct(durations)\n",
        "distincts = [note_names, n_notes, duration_names, n_durations]\n",
        "\n",
        "# make the lookup dictionaries for notes and dictionaries and save\n",
        "note_to_int, int_to_note = create_lookups(note_names)\n",
        "duration_to_int, int_to_duration = create_lookups(duration_names)\n",
        "lookups = [note_to_int, int_to_note, duration_to_int, int_to_duration]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T02:25:42.354609Z",
          "iopub.status.busy": "2022-03-16T02:25:42.354298Z",
          "iopub.status.idle": "2022-03-16T02:25:42.363124Z",
          "shell.execute_reply": "2022-03-16T02:25:42.362388Z",
          "shell.execute_reply.started": "2022-03-16T02:25:42.354569Z"
        },
        "id": "iC0rKjFVD8uw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "len(note_to_int), note_to_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T02:25:42.365081Z",
          "iopub.status.busy": "2022-03-16T02:25:42.364494Z",
          "iopub.status.idle": "2022-03-16T02:25:42.373721Z",
          "shell.execute_reply": "2022-03-16T02:25:42.372962Z",
          "shell.execute_reply.started": "2022-03-16T02:25:42.365038Z"
        },
        "id": "2fHx1IGqD8uw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print('\\nduration_to_int')\n",
        "len(duration_to_int),duration_to_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T02:25:42.375862Z",
          "iopub.status.busy": "2022-03-16T02:25:42.375111Z",
          "iopub.status.idle": "2022-03-16T02:25:42.680043Z",
          "shell.execute_reply": "2022-03-16T02:25:42.679309Z",
          "shell.execute_reply.started": "2022-03-16T02:25:42.37582Z"
        },
        "id": "pMZc2nGzD8uw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset = prepare_sequences(notes, durations, lookups, distincts, seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB3eZts1D8uw"
      },
      "source": [
        "Divide the dataset by 32 notes to create the training set. Target is the next pitch and time signature in the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NozL4h6D8ux"
      },
      "source": [
        "----------------------------------\n",
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "network_input, network_output = prepare_sequences(notes, durations, lookups, distincts, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('pitch input')\n",
        "print(network_input[0][0])\n",
        "print('duration input')\n",
        "print(network_input[1][0])\n",
        "print('pitch target')\n",
        "print(network_output[0][0])\n",
        "print('duration target')\n",
        "print(network_output[1][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, att_model = create_network(n_notes, n_durations, embed_size, rnn_units, use_attention)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mF2v9QpD8ux"
      },
      "source": [
        "---------------------------------------------\n",
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(network_input, network_output\n",
        "          , epochs=10, batch_size=64\n",
        "          , validation_split = 0.2\n",
        "          , shuffle=True\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Qu8jGID8ux"
      },
      "source": [
        "----------------------------------------\n",
        "# Predicting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T03:02:11.72986Z",
          "iopub.status.busy": "2022-03-16T03:02:11.729365Z",
          "iopub.status.idle": "2022-03-16T03:02:11.740723Z",
          "shell.execute_reply": "2022-03-16T03:02:11.739775Z",
          "shell.execute_reply.started": "2022-03-16T03:02:11.729829Z"
        },
        "id": "k_qROzvvD8ux",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# prediction params\n",
        "notes_temp=0.9\n",
        "duration_temp = 0.9\n",
        "max_extra_notes = 210\n",
        "max_seq_len = 32\n",
        "seq_len = 32\n",
        "\n",
        "notes = ['START']\n",
        "durations = [0]\n",
        "\n",
        "if seq_len is not None:\n",
        "    notes = ['START'] * (seq_len - len(notes)) + notes\n",
        "    durations = [0] * (seq_len - len(durations)) + durations\n",
        "\n",
        "sequence_length = len(notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T03:02:11.742766Z",
          "iopub.status.busy": "2022-03-16T03:02:11.742548Z",
          "iopub.status.idle": "2022-03-16T03:02:23.672571Z",
          "shell.execute_reply": "2022-03-16T03:02:23.671672Z",
          "shell.execute_reply.started": "2022-03-16T03:02:11.742739Z"
        },
        "id": "TNDW-kQmD8ux",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "prediction_output = []\n",
        "notes_input_sequence = []\n",
        "durations_input_sequence = []\n",
        "overall_preds = []\n",
        "\n",
        "for n, d in zip(notes,durations):\n",
        "    note_int = note_to_int[n]\n",
        "    duration_int = duration_to_int[d]\n",
        "\n",
        "    notes_input_sequence.append(note_int)\n",
        "    durations_input_sequence.append(duration_int)\n",
        "\n",
        "    prediction_output.append([n, d])\n",
        "\n",
        "    if n != 'START':\n",
        "        midi_note = note.Note(n)\n",
        "        new_note = np.zeros(128)\n",
        "        new_note[midi_note.pitch.midi] = 1\n",
        "        overall_preds.append(new_note)\n",
        "\n",
        "att_matrix = np.zeros(shape = (max_extra_notes+sequence_length, max_extra_notes))\n",
        "\n",
        "for note_index in range(max_extra_notes):\n",
        "\n",
        "    prediction_input = [\n",
        "        np.array([notes_input_sequence])\n",
        "        , np.array([durations_input_sequence])\n",
        "       ]\n",
        "\n",
        "    notes_prediction, durations_prediction = model.predict(prediction_input, verbose=0)\n",
        "    if use_attention:\n",
        "        att_prediction = att_model.predict(prediction_input, verbose=0)[0]\n",
        "        att_matrix[(note_index-len(att_prediction)+sequence_length):(note_index+sequence_length), note_index] = att_prediction\n",
        "\n",
        "    new_note = np.zeros(128)\n",
        "\n",
        "    for idx, n_i in enumerate(notes_prediction[0]):\n",
        "        try:\n",
        "            note_name = int_to_note[idx]\n",
        "            midi_note = note.Note(note_name)\n",
        "            new_note[midi_note.pitch.midi] = n_i\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    overall_preds.append(new_note)\n",
        "\n",
        "    i1 = sample_with_temp(notes_prediction[0], notes_temp)\n",
        "    i2 = sample_with_temp(durations_prediction[0], duration_temp)\n",
        "\n",
        "    note_result = int_to_note[i1]\n",
        "    duration_result = int_to_duration[i2]\n",
        "\n",
        "    prediction_output.append([note_result, duration_result])\n",
        "\n",
        "    notes_input_sequence.append(i1)\n",
        "    durations_input_sequence.append(i2)\n",
        "\n",
        "    if len(notes_input_sequence) > max_seq_len:\n",
        "        notes_input_sequence = notes_input_sequence[1:]\n",
        "        durations_input_sequence = durations_input_sequence[1:]\n",
        "\n",
        "    if note_result == 'START':\n",
        "        break\n",
        "\n",
        "overall_preds = np.transpose(np.array(overall_preds))\n",
        "print('Generated sequence of {} notes'.format(len(prediction_output)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T03:02:24.634201Z",
          "iopub.status.busy": "2022-03-16T03:02:24.633955Z",
          "iopub.status.idle": "2022-03-16T03:02:24.843924Z",
          "shell.execute_reply": "2022-03-16T03:02:24.843343Z",
          "shell.execute_reply.started": "2022-03-16T03:02:24.634164Z"
        },
        "id": "OFk5IuVuD8u8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "midi_stream = stream.Stream()\n",
        "\n",
        "# create note and chord objects based on the values generated by the model\n",
        "for pattern in prediction_output:\n",
        "    note_pattern, duration_pattern = pattern\n",
        "    # pattern is a chord\n",
        "    if ('.' in note_pattern):\n",
        "        notes_in_chord = note_pattern.split('.')\n",
        "        chord_notes = []\n",
        "        for current_note in notes_in_chord:\n",
        "            new_note = note.Note(current_note)\n",
        "            new_note.duration = duration.Duration(duration_pattern)\n",
        "            new_note.storedInstrument = instrument.Violoncello()\n",
        "            chord_notes.append(new_note)\n",
        "        new_chord = chord.Chord(chord_notes)\n",
        "        midi_stream.append(new_chord)\n",
        "    elif note_pattern == 'rest':\n",
        "    # pattern is a rest\n",
        "        new_note = note.Rest()\n",
        "        new_note.duration = duration.Duration(duration_pattern)\n",
        "        new_note.storedInstrument = instrument.Violoncello()\n",
        "        midi_stream.append(new_note)\n",
        "    elif note_pattern != 'START':\n",
        "    # pattern is a note\n",
        "        new_note = note.Note(note_pattern)\n",
        "        new_note.duration = duration.Duration(duration_pattern)\n",
        "        new_note.storedInstrument = instrument.Violoncello()\n",
        "        midi_stream.append(new_note)\n",
        "\n",
        "midi_stream = midi_stream.chordify()\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "new_file = 'output-' + timestr + '.mid'\n",
        "midi_stream.write('midi', new_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T03:02:24.845595Z",
          "iopub.status.busy": "2022-03-16T03:02:24.844868Z",
          "iopub.status.idle": "2022-03-16T03:02:31.864417Z",
          "shell.execute_reply": "2022-03-16T03:02:31.863342Z",
          "shell.execute_reply.started": "2022-03-16T03:02:24.845562Z"
        },
        "id": "yPZlhcRbD8u8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fs = FluidSynth()\n",
        "fs.midi_to_audio(new_file, 'new_output.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T03:02:57.437716Z",
          "iopub.status.busy": "2022-03-16T03:02:57.437236Z",
          "iopub.status.idle": "2022-03-16T03:02:57.473984Z",
          "shell.execute_reply": "2022-03-16T03:02:57.472765Z",
          "shell.execute_reply.started": "2022-03-16T03:02:57.437671Z"
        },
        "id": "IzAc7BdiD8u9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(\"new_output.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m8koIBED8u9"
      },
      "source": [
        "# Saving and Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DthYM3tLD8u9"
      },
      "source": [
        "<hr style=\"border: solid 3px blue;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4aZoz4SKD8u9"
      },
      "outputs": [],
      "source": [
        "# Save the main model using the new .keras format\n",
        "model.save('music_generation_model.keras')\n",
        "\n",
        "# If you're using attention, save the attention model as well\n",
        "if use_attention:\n",
        "    att_model.save('music_generation_attention_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the main model\n",
        "model = load_model('music_generation_model.keras', safe_mode=False, custom_objects={'helper_function': helper_function,'tf':tf})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "[Music Generation] Let's enjoy new music!",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "limewire",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
