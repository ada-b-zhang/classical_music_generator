{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3585d77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  style=\"background-color: #c78cf5; color: black;\">  \n",
    "  <h1>About</h1>\n",
    "  Try music transformer. Make sure to use the `classical_music_generator_env` environment. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43494972",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  style=\"background-color: #c78cf5; color: black;\">  \n",
    "  <h1>Load Stuff</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71445970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI\n",
    "from miditok import REMI, TokSequence\n",
    "from miditoolkit import MidiFile\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad19007",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_dir = Path(\"sample_midi\")\n",
    "token_output_dir = Path(\"tokenized_midi\")\n",
    "token_output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37841c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  style=\"background-color: #c78cf5; color: black;\">  \n",
    "  <h1>Prepare for Training</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fa5397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/pzs88x3s1fv95pbq0f9_m8dm0000gn/T/ipykernel_1730/2679437699.py:17: UserWarning: You are using a depreciated `miditoolkit.MidiFile` object. MidiTokis now (>v3.0.0) using symusic.Score as MIDI backend. Your file willbe converted on the fly, however please consider using symusic.\n",
      "  token_seq = tokenizer(midi)\n"
     ]
    }
   ],
   "source": [
    "label_map = {\n",
    "    \"air.mid\": {\"composer\": \"<composer:Bach>\", \"era\": \"<era:Baroque>\"},\n",
    "    \"furelise.mid\": {\"composer\": \"<composer:Beethoven>\", \"era\": \"<era:Classical>\"}\n",
    "}\n",
    "\n",
    "tokenizer = REMI()\n",
    "midi_paths = list(midi_dir.glob(\"*.mid\"))\n",
    "\n",
    "for midi_path in midi_paths:\n",
    "    filename = midi_path.name\n",
    "    if filename not in label_map:\n",
    "        print(f\"Skipping unlabeled file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    labels = label_map[filename]\n",
    "    midi = MidiFile(str(midi_path))\n",
    "    token_seq = tokenizer(midi)\n",
    "\n",
    "    # Assume 1 segment per file\n",
    "    styled_seq = [labels[\"composer\"], labels[\"era\"]] + token_seq[0].tokens\n",
    "\n",
    "    with open(token_output_dir / f\"{filename}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(styled_seq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fb7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# Setup & Token ID Conversion\n",
    "##########################################################################################\n",
    "\n",
    "# Load tokenized sequences\n",
    "input_dir = Path(\"tokenized_midi\")\n",
    "files = list(input_dir.glob(\"*.pkl\"))\n",
    "\n",
    "styled_tokens = []\n",
    "for f in files:\n",
    "    with open(f, \"rb\") as infile:\n",
    "        styled_tokens.append(pickle.load(infile))\n",
    "\n",
    "# Build vocab\n",
    "all_tokens = set(token for seq in styled_tokens for token in seq)\n",
    "token2id = {tok: i for i, tok in enumerate(sorted(all_tokens))}\n",
    "id2token = {i: tok for tok, i in token2id.items()}\n",
    "\n",
    "# Add pad token\n",
    "pad_token = \"<PAD>\"\n",
    "pad_token_id = len(token2id)\n",
    "token2id[pad_token] = pad_token_id\n",
    "id2token[pad_token_id] = pad_token\n",
    "\n",
    "# Convert tokens to IDs\n",
    "token_ids = [[token2id[tok] for tok in seq] for seq in styled_tokens]\n",
    "\n",
    "# Wrap in HF Dataset\n",
    "examples = [{\"input_ids\": ids} for ids in token_ids]\n",
    "hf_dataset = Dataset.from_list(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd0930",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  style=\"background-color: #c78cf5; color: black;\">  \n",
    "  <h1>Model</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a17037",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "# Define Model\n",
    "##########################################################################################\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(token2id),\n",
    "    n_positions=1024,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    pad_token_id=pad_token_id\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0521fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.237700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=2.6823679924011232, metrics={'train_runtime': 25.7607, 'train_samples_per_second': 1.553, 'train_steps_per_second': 0.776, 'total_flos': 16977790771200.0, 'train_loss': 2.6823679924011232, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################################\n",
    "# Fine-tuning\n",
    "##########################################################################################\n",
    "class SimpleDataCollator:\n",
    "    def __call__(self, examples):\n",
    "        input_ids = [torch.tensor(e[\"input_ids\"], dtype=torch.long) for e in examples]\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "        labels = input_ids.clone()\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_steps=100,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "collator = SimpleDataCollator()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2dbe53",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  style=\"background-color: #c78cf5; color: black;\">  \n",
    "  <h1>Music Generation</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a058c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################## \n",
    "# Generate Music\n",
    "##########################################################################################\n",
    "start = [\"<composer:Bach>\", \"<era:Baroque>\"]\n",
    "start_ids = [token2id[tok] for tok in start]\n",
    "input_ids = torch.tensor(start_ids).unsqueeze(0)\n",
    "\n",
    "# Move model & input to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "generated_ids = output[0].tolist()\n",
    "generated_tokens = [id2token[i] for i in generated_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2687a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################## \n",
    "# Convert to MIDI\n",
    "##########################################################################################\n",
    "\n",
    "# Re-init tokenizer\n",
    "tokenizer = REMI()\n",
    "\n",
    "# Remove style tokens like <composer:*>, <era:*>\n",
    "music_tokens = [tok for tok in generated_tokens if \"_\" in tok]\n",
    "\n",
    "# Wrap in TokSequence\n",
    "tok_seq = TokSequence(tokens=music_tokens)\n",
    "\n",
    "# Decode back to a symusic.Score/ScoreTick object\n",
    "score = tokenizer.decode([tok_seq])\n",
    "\n",
    "# Save to .mid using dump_midi()\n",
    "score.dump_midi(\"generations/generated_classical.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803df804",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  style=\"background-color: #c78cf5; color: black;\">  \n",
    "  <h1>Try it Out</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac6c67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classical_midi(composer: str, era: str, filename=\"generated_piece.mid\"):\n",
    "    from miditok import REMI, TokSequence\n",
    "    import torch\n",
    "\n",
    "    # Initialize tokenizer + model device\n",
    "    tokenizer = REMI()\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare input tokens\n",
    "    prompt_tokens = [f\"<composer:{composer}>\", f\"<era:{era}>\"]\n",
    "    prompt_ids = [token2id[t] for t in prompt_tokens]\n",
    "    input_ids = torch.tensor(prompt_ids).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=512,\n",
    "        do_sample=True,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    # Convert IDs to tokens\n",
    "    generated_ids = output[0].tolist()\n",
    "    generated_tokens = [id2token[i] for i in generated_ids]\n",
    "\n",
    "    # Filter out label tokens\n",
    "    music_tokens = [tok for tok in generated_tokens if \"_\" in tok]\n",
    "\n",
    "    # Decode to MIDI\n",
    "    tok_seq = TokSequence(tokens=music_tokens)\n",
    "    score = tokenizer.decode([tok_seq])\n",
    "    score.dump_midi(filename)\n",
    "\n",
    "    print(f\"ðŸŽµ Saved generated piece to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141226c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽµ Saved generated piece to generations/bach_baroque.mid\n",
      "ðŸŽµ Saved generated piece to generations/beethoven_classical.mid\n"
     ]
    }
   ],
   "source": [
    "generate_classical_midi(\"Bach\", \"Baroque\", filename=\"generations/bach_baroque.mid\")\n",
    "generate_classical_midi(\"Beethoven\", \"Classical\", filename=\"generations/beethoven_classical.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60787602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classical_music_generator_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
